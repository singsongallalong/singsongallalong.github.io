---
layout: meme.njk
title: "attention"
permalink: /meme/{{ page.fileSlug }}/
description: Position independent dependency 
concepts: []
---
	
[[Convolution]]s are translation invariant: they produce features which depend on nearby values in the input data in a linear, spatialy homogeneous way, meaning the value of a specific filter at $y_i$ is a linear function of nearby values $x_j, i-n \leq j \leq i+n$, but the filter can't change as we move over the sequence. Since this function is a linear map $w : \mathbb{R}^{2n+1} \rightarrow \mathbb{R}$, it can be represented by a $1 \times (2n+1)$ matrix $[w]_{ij}$ relative to the standard basis: a row vector, where the matrix multiplication representing the linear action $wv$ is given by a dot product against $[w]_{1j}$. Translation invariance just means that this vector, which is the kernel of the convolution, is the same for every $y_i$. This is useful because it approximately describes many natural operators very well and it means you only need to keep track of the kernel and a bias, rather than the massive size a dense layer would have. Differentiation does not care where you are, it only cares what nearby values look like, and it is linear, so it may be represented as a convolution (against $-\delta'(x)$: you can get a more interesting representation by using a sequence of smooth functions approximating $\delta(x)$ trying to differentiate under the limit sign). The solution to any linear PDE with constant coefficients is also easily treatable in terms of convolutions as well for similar reasons, and constant coefficient approximations for PDEs with spatially varying coefficients can work in some cases, so people will often opt for a constant coefficient approximation if they expect e.g. the heat-conducting properties of a material to be roughly homogeneous. So for any kind of spatial, physical signal, convolutions, despite their simplicity, actually are very expressive. As an additional large caveat the kernels in ML are finite and usually small [[small-kernels]]. That means that many convolutional operations like inverting differential operators should be off the table. However in practice finite approximations can work pretty well especially in a situation where you are grossly overparameterizing the function you are trying to approximate, so even if we can't apply the Poisson kernel exactly, we could get something very close in two or three layers, and especially close insofar as we particularly care about. But regardless of how large we can get our receptive field, convolutions will always be translation invariant. They are not well suited to modeling spatially inhomogeneous relationships where we care a great lot about something happening in a far away place more than we care about something happening nearby. Let's say there's an important piece of data that every part of the output should care about. You'd need a kernel larger enough to 1. find the spike, 2. taking on sufficiently large values near that piece of data to amplify it. But unless that piece of data is disproportionately large compared to the nearby data, then if we move three steps to the right in our output data, we'll care just as much about the nearby data three steps to the right of the "important-piece" in the input as we did about the important piece before we took three steps to the right in our output. Consider the figure below.

![Convolution downsides](/images/convolution_downsides.png)

However natural language is quite a different beast where we do not have similar reason to believe that operations which are 'like' applying the heat operator to a function, or applying the heat kernel, etc, would be inherently useful. 
